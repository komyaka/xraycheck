# Отдельный workflow: те же проверки, что и daily-check, с актуализацией configs/notworkers.
# Рабочие ключи из notworkers добавляются к существующим available и available(top100) (без перезаписи), нерабочие пополняют notworkers.
# TTL: ключи в notworkers хранятся только NOTWORKERS_TTL_DAYS дней (даты в configs/notworkers_ttl.json), старые удаляются перед проверкой.

name: Actualize notworkers

on:
  schedule:
    # Дважды в день (UTC), между запусками daily-check (00, 04, 08, 12, 16, 20)
    - cron: '10 2 * * *'
    - cron: '10 14 * * *'
  workflow_dispatch:
    # Ручной запуск из вкладки Actions

concurrency:
  group: notworkers-actualize
  cancel-in-progress: true

env:
  MODE: notworkers
  OUTPUT_FILE: available
  OUTPUT_DIR: configs
  OUTPUT_ADD_DATE: 'false'
  MAX_WORKERS: 600
  EXPORT_FORMAT: txt
  ENABLE_CACHE: 'false'
  TEST_URLS: http://www.google.com/generate_204,http://www.cloudflare.com/cdn-cgi/trace
  TEST_URLS_HTTPS: https://www.gstatic.com/generate_204
  REQUIRE_HTTPS: 'true'
  STRONG_STYLE_TEST: 'true'
  STRONG_STYLE_TIMEOUT: 12
  STRONG_MAX_RESPONSE_TIME: 3
  STRONG_DOUBLE_CHECK: 'true'
  STRONG_ATTEMPTS: 3
  REQUESTS_PER_URL: 2
  MIN_SUCCESSFUL_REQUESTS: 2
  MIN_SUCCESSFUL_URLS: 2
  REQUEST_DELAY: 0.1
  CONNECT_TIMEOUT: 6
  CONNECT_TIMEOUT_SLOW: 15
  USE_ADAPTIVE_TIMEOUT: 'false'
  MAX_RETRIES: 1
  MAX_RESPONSE_TIME: 6
  MIN_RESPONSE_SIZE: 0
  MAX_LATENCY_MS: 2000
  VERIFY_HTTPS_SSL: 'false'
  STABILITY_CHECKS: 2
  STABILITY_CHECK_DELAY: 2.0
  STRICT_MODE: 'true'
  STRICT_MODE_REQUIRE_ALL: 'true'
  TEST_POST_REQUESTS: 'false'

  # Speedtest (после checker: результат в available и available(top100))
  SPEED_TEST_ENABLED: 'true'
  SPEED_TEST_TIMEOUT: 2
  SPEED_TEST_MODE: full
  SPEED_TEST_METRIC: latency
  SPEED_TEST_OUTPUT: separate_file
  SPEED_TEST_REQUESTS: 5
  SPEED_TEST_URL: https://www.gstatic.com/generate_204
  SPEED_TEST_WORKERS: 600
  SPEED_TEST_DOWNLOAD_TIMEOUT: 30
  SPEED_TEST_DOWNLOAD_URL_SMALL: https://speed.cloudflare.com/__down?bytes=250000
  SPEED_TEST_DOWNLOAD_URL_MEDIUM: https://speed.cloudflare.com/__down?bytes=1000000
  MIN_SPEED_THRESHOLD_MBPS: 1
  SPEED_TEST_DEBUG: 'false'
  XRAY_STARTUP_WAIT: 1.8
  XRAY_STARTUP_POLL_INTERVAL: 0.2
  BASE_PORT: 20000

  # TTL для notworkers: ключ считается нерабочим только N дней, потом удаляется из списка
  NOTWORKERS_TTL_DAYS: 3

jobs:
  check-and-actualize-notworkers:
    runs-on: ubuntu-latest
    timeout-minutes: 50
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Expire old notworkers (TTL)
        run: |
          python -c "
          import os
          import sys
          import json
          from datetime import datetime, timedelta
          sys.path.insert(0, os.getcwd())
          from lib.parsing import load_notworkers_with_lines, save_notworkers

          configs = 'configs'
          notworkers_path = os.path.join(configs, 'notworkers')
          ttl_path = os.path.join(configs, 'notworkers_ttl.json')
          ttl_days = int(os.environ.get('NOTWORKERS_TTL_DAYS', '30'))

          if not os.path.isfile(notworkers_path):
              print('No notworkers file, skip TTL expire')
              exit(0)

          existing_set, normalized_to_full = load_notworkers_with_lines(notworkers_path)
          ttl = {}
          if os.path.isfile(ttl_path):
              try:
                  with open(ttl_path, encoding='utf-8') as f:
                      ttl = json.load(f)
              except Exception:
                  ttl = {}

          cutoff = (datetime.utcnow() - timedelta(days=ttl_days)).strftime('%Y-%m-%d')
          kept = {}
          removed = 0
          for norm, full in normalized_to_full.items():
              date_str = ttl.get(norm)
              if date_str is None or date_str >= cutoff:
                  kept[norm] = full
                  if date_str:
                      ttl[norm] = date_str
              else:
                  removed += 1

          save_notworkers(notworkers_path, kept)
          os.makedirs(configs, exist_ok=True)
          kept_ttl = {n: ttl.get(n) or datetime.utcnow().strftime('%Y-%m-%d') for n in kept}
          with open(ttl_path, 'w', encoding='utf-8') as f:
              json.dump(kept_ttl, f, ensure_ascii=False, indent=0)
          print(f'TTL expire: removed {removed} entries older than {ttl_days} days, kept {len(kept)}')
          "
        env:
          NOTWORKERS_TTL_DAYS: ${{ env.NOTWORKERS_TTL_DAYS }}

      - name: Backup existing available and available(top100)
        run: |
          mkdir -p configs
          if [ -f "configs/available" ] && [ -s "configs/available" ]; then
            cp "configs/available" "configs/available_backup"
            echo "Backed up configs/available"
          fi
          if [ -f "configs/available(top100)" ] && [ -s "configs/available(top100)" ]; then
            cp "configs/available(top100)" "configs/available_top100_backup"
            echo "Backed up configs/available(top100)"
          fi

      - name: Run vless_checker.py (check keys from configs/notworkers only, write available)
        run: python vless_checker.py

      - name: Update TTL dates for notworkers
        run: |
          python -c "
          import os
          import sys
          import json
          from datetime import datetime
          sys.path.insert(0, os.getcwd())
          from lib.parsing import load_notworkers_with_lines

          configs = 'configs'
          notworkers_path = os.path.join(configs, 'notworkers')
          ttl_path = os.path.join(configs, 'notworkers_ttl.json')

          if not os.path.isfile(notworkers_path):
              print('No notworkers file, skip TTL update')
              exit(0)

          existing_set, _ = load_notworkers_with_lines(notworkers_path)
          today = datetime.utcnow().strftime('%Y-%m-%d')
          ttl = {norm: today for norm in existing_set}
          os.makedirs(configs, exist_ok=True)
          with open(ttl_path, 'w', encoding='utf-8') as f:
              json.dump(ttl, f, ensure_ascii=False, indent=0)
          print(f'TTL dates updated for {len(ttl)} keys (date={today})')
          "

      - name: Run speedtest on configs/available
        if: env.SPEED_TEST_ENABLED == 'true'
        run: python speedtest_checker.py configs/available
        env:
          MAX_WORKERS: ${{ env.MAX_WORKERS }}
          BASE_PORT: ${{ env.BASE_PORT }}
          XRAY_STARTUP_WAIT: ${{ env.XRAY_STARTUP_WAIT }}
          XRAY_STARTUP_POLL_INTERVAL: ${{ env.XRAY_STARTUP_POLL_INTERVAL }}
          VERIFY_HTTPS_SSL: ${{ env.VERIFY_HTTPS_SSL }}
          SPEED_TEST_ENABLED: ${{ env.SPEED_TEST_ENABLED }}
          SPEED_TEST_TIMEOUT: ${{ env.SPEED_TEST_TIMEOUT }}
          SPEED_TEST_MODE: ${{ env.SPEED_TEST_MODE }}
          SPEED_TEST_METRIC: ${{ env.SPEED_TEST_METRIC }}
          SPEED_TEST_OUTPUT: ${{ env.SPEED_TEST_OUTPUT }}
          SPEED_TEST_REQUESTS: ${{ env.SPEED_TEST_REQUESTS }}
          SPEED_TEST_URL: ${{ env.SPEED_TEST_URL }}
          SPEED_TEST_WORKERS: ${{ env.SPEED_TEST_WORKERS }}
          SPEED_TEST_DOWNLOAD_TIMEOUT: ${{ env.SPEED_TEST_DOWNLOAD_TIMEOUT }}
          SPEED_TEST_DOWNLOAD_URL_SMALL: ${{ env.SPEED_TEST_DOWNLOAD_URL_SMALL }}
          SPEED_TEST_DOWNLOAD_URL_MEDIUM: ${{ env.SPEED_TEST_DOWNLOAD_URL_MEDIUM }}
          MIN_SPEED_THRESHOLD_MBPS: ${{ env.MIN_SPEED_THRESHOLD_MBPS }}
          SPEED_TEST_DEBUG: ${{ env.SPEED_TEST_DEBUG }}
          OUTPUT_DIR: configs

      - name: Merge new working keys into available (append, dedupe by key)
        run: |
          python -c "
          import os
          import sys
          sys.path.insert(0, os.getcwd())
          from lib.parsing import normalize_proxy_link

          def read_lines(path):
              if not os.path.isfile(path) or os.path.getsize(path) == 0:
                  return []
              with open(path, encoding='utf-8') as f:
                  return [line.rstrip('\n') for line in f if line.strip() and not line.strip().startswith('#')]

          def link_from_line(line):
              return line.strip().split(maxsplit=1)[0].strip()

          configs = 'configs'
          backup = os.path.join(configs, 'available_backup')
          backup_top = os.path.join(configs, 'available_top100_backup')
          new_from_speed = os.path.join(configs, 'available_st')
          new_top_speed = os.path.join(configs, 'available_st(top100)')
          checker_out = os.path.join(configs, 'available')
          out_path = os.path.join(configs, 'available')
          out_top = os.path.join(configs, 'available(top100)')

          # New keys: from speedtest result if present, else from checker output
          if os.path.isfile(new_from_speed) and os.path.getsize(new_from_speed) > 0:
              new_lines = read_lines(new_from_speed)
              new_top_lines = read_lines(new_top_speed) if os.path.isfile(new_top_speed) else new_lines[:100]
          else:
              new_lines = read_lines(checker_out)
              new_top_lines = new_lines[:100]

          existing_lines = read_lines(backup) if os.path.isfile(backup) else []
          existing_top = read_lines(backup_top) if os.path.isfile(backup_top) else []

          seen = set()
          merged = []
          for line in existing_lines:
              link = link_from_line(line)
              if not link:
                  continue
              norm = normalize_proxy_link(link)
              if norm and norm not in seen:
                  seen.add(norm)
                  merged.append(line)
          for line in new_lines:
              link = link_from_line(line)
              if not link:
                  continue
              norm = normalize_proxy_link(link)
              if norm and norm not in seen:
                  seen.add(norm)
                  merged.append(line)

          os.makedirs(configs, exist_ok=True)
          with open(out_path, 'w', encoding='utf-8') as f:
              f.write('\n'.join(merged))
          print(f'Merged available: {len(existing_lines)} existing + new - {len(merged)} total')

          top_seen = set()
          top100 = []
          for line in new_top_lines:
              if len(top100) >= 100:
                  break
              link = link_from_line(line)
              if not link:
                  continue
              norm = normalize_proxy_link(link)
              if norm and norm not in top_seen:
                  top_seen.add(norm)
                  top100.append(line)
          for line in existing_top:
              if len(top100) >= 100:
                  break
              link = link_from_line(line)
              if not link:
                  continue
              norm = normalize_proxy_link(link)
              if norm and norm not in top_seen:
                  top_seen.add(norm)
                  top100.append(line)

          with open(out_top, 'w', encoding='utf-8') as f:
              f.write('\n'.join(top100))
          print(f'Merged available(top100): {len(top100)} keys')
          "

      - name: Commit and push configs/available, configs/available(top100), configs/notworkers and configs/notworkers_ttl.json
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          mkdir -p configs
          if [ -f "configs/available" ]; then git add configs/available; fi
          if [ -f "configs/available(top100)" ]; then git add "configs/available(top100)"; fi
          if [ -f "configs/notworkers" ]; then git add configs/notworkers; fi
          if [ -f "configs/notworkers_ttl.json" ]; then git add configs/notworkers_ttl.json; fi
          if ! git diff --cached --quiet; then
            NOW=$(date -u +%Y-%m-%dT%H:%M:%SZ)
            C=$(wc -l < "configs/available" 2>/dev/null || echo "0")
            C100=$(wc -l < "configs/available(top100)" 2>/dev/null || echo "0")
            OLD=$(cat configs/last-updated.json 2>/dev/null || echo '{}')
            echo "$OLD" | jq -c --arg t "$NOW" --argjson c "$C" --argjson c100 "$C100" '.["configs/available"] = {updated: $t, count: $c} | .["configs/available(top100)"] = {updated: $t, count: $c100}' > configs/last-updated.json
            git add configs/last-updated.json
          fi
          if ! git diff --cached --quiet; then
            git commit -m "actualize notworkers, update available and top100 [check + speedtest, automated]"
            git pull --no-rebase origin "${{ github.ref_name }}"
            if [ -f .git/MERGE_HEAD ]; then
              if grep -q '^<<<<<<<' configs/last-updated.json 2>/dev/null; then
                git checkout --theirs configs/last-updated.json
                OLD=$(cat configs/last-updated.json 2>/dev/null || echo '{}')
                NOW=$(date -u +%Y-%m-%dT%H:%M:%SZ)
                C=$(wc -l < "configs/available" 2>/dev/null || echo "0")
                C100=$(wc -l < "configs/available(top100)" 2>/dev/null || echo "0")
                echo "$OLD" | jq -c --arg t "$NOW" --argjson c "$C" --argjson c100 "$C100" '.["configs/available"] = {updated: $t, count: $c} | .["configs/available(top100)"] = {updated: $t, count: $c100}' > configs/last-updated.json
                git add configs/last-updated.json
              fi
              git commit -m "Merge remote; resolve last-updated.json"
            fi
            git push
          else
            echo "No changes, skip push"
          fi
